{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6cba1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(90000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 90 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8fd21",
   "metadata": {},
   "source": [
    "Name: Jishnu Chander Ravichanderan <br>\n",
    "Github Username: jishnuchander <br>\n",
    "USC ID: 1144843551 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ed475c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor as fl\n",
    "import seaborn as sns\n",
    "from scipy.stats import variation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, hamming_loss, silhouette_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss, mean_squared_error, roc_auc_score \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from xgboost import  XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.spatial.distance import hamming\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d6327",
   "metadata": {},
   "source": [
    "(b) Each instance has three labels: Families, Genus, and Species. Each of the labels\n",
    "has multiple classes. We wish to solve a multi-class and multi-label problem.\n",
    "One of the most important approaches to multi-label classification is to train a\n",
    "classifier for each label (binary relevance). We first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04229a7",
   "metadata": {},
   "source": [
    "i. Research exact match and hamming score/ loss methods for evaluating multi-\n",
    "label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb7dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()+\"\\Homework_7_Data\\\\Anuran_Calls\\\\Frogs_MFCCs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cab780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "      <th>Family</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>RecordID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>-0.105586</td>\n",
       "      <td>0.200722</td>\n",
       "      <td>0.317201</td>\n",
       "      <td>0.260764</td>\n",
       "      <td>0.100945</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>-0.171128</td>\n",
       "      <td>0.124676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108351</td>\n",
       "      <td>-0.077623</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.057684</td>\n",
       "      <td>0.118680</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171534</td>\n",
       "      <td>-0.098975</td>\n",
       "      <td>0.268425</td>\n",
       "      <td>0.338672</td>\n",
       "      <td>0.268353</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>-0.222475</td>\n",
       "      <td>-0.207693</td>\n",
       "      <td>0.170883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090974</td>\n",
       "      <td>-0.056510</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.082973</td>\n",
       "      <td>0.287128</td>\n",
       "      <td>0.276014</td>\n",
       "      <td>0.189867</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>-0.242234</td>\n",
       "      <td>-0.219153</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050691</td>\n",
       "      <td>-0.023590</td>\n",
       "      <td>-0.066722</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.077162</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224392</td>\n",
       "      <td>0.118985</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>-0.194347</td>\n",
       "      <td>-0.098181</td>\n",
       "      <td>0.270375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136009</td>\n",
       "      <td>-0.177037</td>\n",
       "      <td>-0.130498</td>\n",
       "      <td>-0.054766</td>\n",
       "      <td>-0.018691</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>-0.068345</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.249144</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>-0.265423</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>0.266434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048885</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>-0.088550</td>\n",
       "      <td>-0.031346</td>\n",
       "      <td>0.108610</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.554504</td>\n",
       "      <td>-0.337717</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.443451</td>\n",
       "      <td>0.093889</td>\n",
       "      <td>-0.100753</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>0.081075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.071001</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>-0.021860</td>\n",
       "      <td>-0.079860</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.517273</td>\n",
       "      <td>-0.370574</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.402890</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>-0.116460</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>-0.015418</td>\n",
       "      <td>-0.101892</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.582557</td>\n",
       "      <td>-0.343237</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.385596</td>\n",
       "      <td>0.114905</td>\n",
       "      <td>-0.103317</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.080425</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.519497</td>\n",
       "      <td>-0.307553</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.086866</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>0.089316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051796</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.041803</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>-0.096895</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.508833</td>\n",
       "      <td>-0.324106</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.078211</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.094596</td>\n",
       "      <td>-0.117672</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.076180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.072983</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.029355</td>\n",
       "      <td>-0.087910</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7195 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "0          1.0  0.152936 -0.105586  0.200722  0.317201  0.260764  0.100945   \n",
       "1          1.0  0.171534 -0.098975  0.268425  0.338672  0.268353  0.060835   \n",
       "2          1.0  0.152317 -0.082973  0.287128  0.276014  0.189867  0.008714   \n",
       "3          1.0  0.224392  0.118985  0.329432  0.372088  0.361005  0.015501   \n",
       "4          1.0  0.087817 -0.068345  0.306967  0.330923  0.249144  0.006884   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7190       1.0 -0.554504 -0.337717  0.035533  0.034511  0.443451  0.093889   \n",
       "7191       1.0 -0.517273 -0.370574  0.030673  0.068097  0.402890  0.096628   \n",
       "7192       1.0 -0.582557 -0.343237  0.029468  0.064179  0.385596  0.114905   \n",
       "7193       1.0 -0.519497 -0.307553 -0.004922  0.072865  0.377131  0.086866   \n",
       "7194       1.0 -0.508833 -0.324106  0.062068  0.078211  0.397188  0.094596   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_17  MFCCs_18  MFCCs_19  \\\n",
       "0    -0.150063 -0.171128  0.124676  ... -0.108351 -0.077623 -0.009568   \n",
       "1    -0.222475 -0.207693  0.170883  ... -0.090974 -0.056510 -0.035303   \n",
       "2    -0.242234 -0.219153  0.232538  ... -0.050691 -0.023590 -0.066722   \n",
       "3    -0.194347 -0.098181  0.270375  ... -0.136009 -0.177037 -0.130498   \n",
       "4    -0.265423 -0.172700  0.266434  ... -0.048885 -0.053074 -0.088550   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7190 -0.100753  0.037087  0.081075  ...  0.069430  0.071001  0.021591   \n",
       "7191 -0.116460  0.063727  0.089034  ...  0.061127  0.068978  0.017745   \n",
       "7192 -0.103317  0.070370  0.081317  ...  0.082474  0.077771 -0.009688   \n",
       "7193 -0.115799  0.056979  0.089316  ...  0.051796  0.069073  0.017963   \n",
       "7194 -0.117672  0.058874  0.076180  ...  0.061455  0.072983 -0.003980   \n",
       "\n",
       "      MFCCs_20  MFCCs_21  MFCCs_22           Family      Genus  \\\n",
       "0     0.057684  0.118680  0.014038  Leptodactylidae  Adenomera   \n",
       "1     0.020140  0.082263  0.029056  Leptodactylidae  Adenomera   \n",
       "2    -0.025083  0.099108  0.077162  Leptodactylidae  Adenomera   \n",
       "3    -0.054766 -0.018691  0.023954  Leptodactylidae  Adenomera   \n",
       "4    -0.031346  0.108610  0.079244  Leptodactylidae  Adenomera   \n",
       "...        ...       ...       ...              ...        ...   \n",
       "7190  0.052449 -0.021860 -0.079860          Hylidae     Scinax   \n",
       "7191  0.046461 -0.015418 -0.101892          Hylidae     Scinax   \n",
       "7192  0.027834 -0.000531 -0.080425          Hylidae     Scinax   \n",
       "7193  0.041803 -0.027911 -0.096895          Hylidae     Scinax   \n",
       "7194  0.031560 -0.029355 -0.087910          Hylidae     Scinax   \n",
       "\n",
       "             Species  RecordID  \n",
       "0     AdenomeraAndre         1  \n",
       "1     AdenomeraAndre         1  \n",
       "2     AdenomeraAndre         1  \n",
       "3     AdenomeraAndre         1  \n",
       "4     AdenomeraAndre         1  \n",
       "...              ...       ...  \n",
       "7190     ScinaxRuber        60  \n",
       "7191     ScinaxRuber        60  \n",
       "7192     ScinaxRuber        60  \n",
       "7193     ScinaxRuber        60  \n",
       "7194     ScinaxRuber        60  \n",
       "\n",
       "[7195 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a80ea1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MFCCs_ 1', 'MFCCs_ 2', 'MFCCs_ 3', 'MFCCs_ 4', 'MFCCs_ 5', 'MFCCs_ 6',\n",
       "       'MFCCs_ 7', 'MFCCs_ 8', 'MFCCs_ 9', 'MFCCs_10', 'MFCCs_11', 'MFCCs_12',\n",
       "       'MFCCs_13', 'MFCCs_14', 'MFCCs_15', 'MFCCs_16', 'MFCCs_17', 'MFCCs_18',\n",
       "       'MFCCs_19', 'MFCCs_20', 'MFCCs_21', 'MFCCs_22', 'Family', 'Genus',\n",
       "       'Species', 'RecordID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4808d69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Leptodactylidae    4420\n",
       "Hylidae            2165\n",
       "Dendrobatidae       542\n",
       "Bufonidae            68\n",
       "Name: Family, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Family'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1223e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc1840dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "le1.fit(df_labels.Family)\n",
    "df_labels['Family'] = le1.transform(df_labels.Family)\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "le2.fit(df_labels.Genus)\n",
    "df_labels['Genus'] = le2.transform(df_labels.Genus)\n",
    "\n",
    "le3 = LabelEncoder()\n",
    "le3.fit(df_labels.Species)\n",
    "df_labels['Species'] = le3.transform(df_labels.Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce9881b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    4420\n",
       "2    2165\n",
       "1     542\n",
       "0      68\n",
       "Name: Family, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels['Family'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c1c7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.drop('RecordID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b01b113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37432776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5036, 25) (2159, 25)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ecf9886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MFCCs_ 1', 'MFCCs_ 2', 'MFCCs_ 3', 'MFCCs_ 4', 'MFCCs_ 5', 'MFCCs_ 6',\n",
       "       'MFCCs_ 7', 'MFCCs_ 8', 'MFCCs_ 9', 'MFCCs_10', 'MFCCs_11', 'MFCCs_12',\n",
       "       'MFCCs_13', 'MFCCs_14', 'MFCCs_15', 'MFCCs_16', 'MFCCs_17', 'MFCCs_18',\n",
       "       'MFCCs_19', 'MFCCs_20', 'MFCCs_21', 'MFCCs_22', 'Family', 'Genus',\n",
       "       'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e815b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Family', 'Genus', 'Species']\n",
    "X_train = train_df.drop(label_columns, axis=1)\n",
    "X_test = test_df.drop(label_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba771c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_1 = train_df['Family']\n",
    "Y_train_2 = train_df['Genus']\n",
    "Y_train_3 = train_df['Species']\n",
    "Y_test_1 = test_df['Family']\n",
    "Y_test_2 = test_df['Genus']\n",
    "Y_test_3 = test_df['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ea3880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = test_df['Family'].tolist()\n",
    "Y_test.extend(test_df['Genus'].tolist())\n",
    "Y_test.extend(test_df['Species'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0798872",
   "metadata": {},
   "source": [
    "ii. Train a SVM for each of the labels, using Gaussian kernels and one versus\n",
    "all classifiers. Determine the weight of the SVM penalty and the width of\n",
    "the Gaussian Kernel using 10 fold cross validation.1 You are welcome to try\n",
    "to solve the problem with both standardized 2 and raw attributes and report\n",
    "the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfa5e3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.7\n",
      "0.9314863464714358\n",
      "0.1 0.9\n",
      "0.9392315419651404\n",
      "0.1 1.1\n",
      "0.9441968559016273\n",
      "0.1 1.3\n",
      "0.949029894705839\n",
      "0.1 1.5\n",
      "0.9534651877097206\n",
      "0.1 1.7\n",
      "0.9582305111132149\n",
      "0.1 1.9\n",
      "0.9619390798068732\n",
      "0.1 2.1\n",
      "0.9638591730043023\n",
      "0.1 2.3\n",
      "0.9659112783615764\n",
      "0.1 2.3\n",
      "1 0.7\n",
      "0.9754435030031454\n",
      "1 0.9\n",
      "0.9812014158435629\n",
      "1 1.1\n",
      "0.9852394889919742\n",
      "1 1.3\n",
      "0.9872906739457434\n",
      "1 1.5\n",
      "0.988283526355097\n",
      "1 1.7\n",
      "0.9890774401211777\n",
      "1 1.9\n",
      "0.9895406660565707\n",
      "1 2.1\n",
      "0.9900701610443161\n",
      "1 2.3\n",
      "0.9904011118474338\n",
      "1 2.3\n",
      "10 0.7\n",
      "0.9905986041423418\n",
      "10 0.9\n",
      "0.9907970168407545\n",
      "10 1.1\n",
      "0.9909959554840272\n",
      "10 1.3\n",
      "0.9911947626410846\n",
      "10 1.5\n",
      "0.9916581200626927\n",
      "10 1.7\n",
      "0.9919897282968855\n",
      "10 1.9\n",
      "0.9922544100476506\n",
      "10 2.1\n",
      "0.992320679100003\n",
      "10 2.3\n",
      "0.9928504370601787\n",
      "10 2.3\n",
      "100 0.7\n",
      "0.9915258449304176\n",
      "100 0.9\n",
      "0.9917245206012602\n",
      "100 1.1\n",
      "0.9917911841122578\n",
      "100 1.3\n",
      "0.9923210735586482\n",
      "100 1.5\n",
      "0.9926526817928408\n",
      "100 1.7\n",
      "0.9926526817928408\n",
      "100 1.9\n",
      "0.9925200122019209\n",
      "100 2.1\n",
      "0.9927845624664711\n",
      "100 2.3\n",
      "0.9929172320573911\n",
      "100 2.3\n"
     ]
    }
   ],
   "source": [
    "# Determining optimal values with raw attributes\n",
    "\n",
    "c_vals = [0.1, 1, 10, 100]\n",
    "gamma_vals = [0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.1, 2.3]\n",
    "\n",
    "best_score = 0\n",
    "svm_hs = []\n",
    "\n",
    "for c in c_vals:\n",
    "    for gamma in gamma_vals:\n",
    "#         smoter = SMOTE(random_state=42)\n",
    "#         print('alpha', alpha)\n",
    "\n",
    "        print(c, gamma)\n",
    "        kf = KFold(n_splits=10)\n",
    "        avg_hs = []\n",
    "\n",
    "        for train_index,  test_index in kf.split(X_train):\n",
    "\n",
    "            x_train_svm, test_x = X_train.iloc[train_index.tolist(),:], X_train.iloc[test_index.tolist(),:]\n",
    "            y_train_1_svm, test_y_1 = np.array(Y_train_1)[train_index.tolist()], np.array(Y_train_1)[test_index.tolist()]\n",
    "            y_train_2_svm, test_y_2 = np.array(Y_train_2)[train_index.tolist()], np.array(Y_train_2)[test_index.tolist()]\n",
    "            y_train_3_svm, test_y_3 = np.array(Y_train_3)[train_index.tolist()], np.array(Y_train_3)[test_index.tolist()]\n",
    "\n",
    "            Y_test = test_y_1.tolist()\n",
    "            Y_test.extend(test_y_2.tolist())\n",
    "            Y_test.extend(test_y_3.tolist())\n",
    "            \n",
    "            clf1 = OneVsRestClassifier(SVC(C=c, gamma=gamma))\n",
    "            clf1.fit(x_train_svm, y_train_1_svm)\n",
    "            y_pred_1_svm = clf1.predict(test_x)\n",
    "#             print(hamming_loss(test_y_1, y_pred_1_svm))\n",
    "\n",
    "            clf2 = OneVsRestClassifier(SVC(C=c, gamma=gamma))\n",
    "            clf2.fit(x_train_svm, y_train_2_svm)\n",
    "            y_pred_2_svm = clf2.predict(test_x)\n",
    "#             print(hamming_loss(test_y_2, y_pred_2_svm))\n",
    "            \n",
    "            clf3 = OneVsRestClassifier(SVC(C=c, gamma=gamma))\n",
    "            clf3.fit(x_train_svm, y_train_3_svm)\n",
    "            y_pred_3_svm = clf3.predict(test_x)\n",
    "#             print(hamming_loss(test_y_3, y_pred_3_svm))\n",
    "\n",
    "            Y_pred = y_pred_1_svm.tolist()\n",
    "            Y_pred.extend(y_pred_2_svm)\n",
    "            Y_pred.extend(y_pred_3_svm)\n",
    "#               X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(x_train_xg, y_train_xg)\n",
    "\n",
    "#             x = XGBClassifier(random_state=42,reg_alpha=alpha, objective='binary:logistic', verbosity=0, n_jobs=-1, eval_metric='logloss')\n",
    "#             xgb.fit(X_train_fold_upsample,y_train_fold_upsample)\n",
    "\n",
    "#             pred = xgb.predict(test_x)\n",
    "#             f1 = f1_score(test_y, pred)\n",
    "        \n",
    "            avg_hs.append(1- hamming_loss(Y_test, Y_pred))\n",
    "\n",
    "        svm_hs.append(np.mean(avg_hs))\n",
    "        print(np.mean(avg_hs))\n",
    "        if np.mean(avg_hs) > best_score:\n",
    "            best_score = np.mean(avg_hs)\n",
    "            best_c = c\n",
    "            best_gamma = gamma\n",
    "\n",
    "    print(best_c, best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d20bc6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C and gamma values are:  100 2.3\n"
     ]
    }
   ],
   "source": [
    "print('The best C and gamma values are: ', best_c, best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f2e7c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the attrubutes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae2bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "886345dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.1\n",
      "0.6033160823419272\n",
      "0.001 0.3\n",
      "0.5966985126279362\n",
      "0.001 0.5\n",
      "0.5937198239136608\n",
      "0.001 0.7\n",
      "0.5916675870701716\n",
      "0.001 0.9\n",
      "0.5912042296485637\n",
      "0.001 1.1\n",
      "0.5912051500520684\n",
      "0.001 1.3\n",
      "0.5909404683013033\n",
      "0.001 1.5\n",
      "0.5901460285903626\n",
      "0.001 0.1\n",
      "0.01 0.1\n",
      "0.8874063818149305\n",
      "0.01 0.3\n",
      "0.7476797942503708\n",
      "0.01 0.5\n",
      "0.6796347575920141\n",
      "0.01 0.7\n",
      "0.6488567273607035\n",
      "0.01 0.9\n",
      "0.6162902216331639\n",
      "0.01 1.1\n",
      "0.596565448578371\n",
      "0.01 1.3\n",
      "0.5910727434335785\n",
      "0.01 1.5\n",
      "0.5901460285903626\n",
      "0.01 0.1\n",
      "0.1 0.1\n",
      "0.9723339855049596\n",
      "0.1 0.3\n",
      "0.9511535285640654\n",
      "0.1 0.5\n",
      "0.8944933573164189\n",
      "0.1 0.7\n",
      "0.8266480482186248\n",
      "0.1 0.9\n",
      "0.7627084056507515\n",
      "0.1 1.1\n",
      "0.7176964667024309\n",
      "0.1 1.3\n",
      "0.6896991069456279\n",
      "0.1 1.5\n",
      "0.673416117054288\n",
      "0.1 0.1\n",
      "1 0.1\n",
      "0.9880191075767616\n",
      "1 0.3\n",
      "0.9789524756224557\n",
      "1 0.5\n",
      "0.9605515057801342\n",
      "1 0.7\n",
      "0.942215227155585\n",
      "1 0.9\n",
      "0.9025700295581013\n",
      "1 1.1\n",
      "0.8679532592802971\n",
      "1 1.3\n",
      "0.8382998306457552\n",
      "1 1.5\n",
      "0.813808813783963\n",
      "1 0.1\n"
     ]
    }
   ],
   "source": [
    "#Determining the optimal values with standardized attributes\n",
    "\n",
    "c_vals = [0.001, 0.01, 0.1, 1]\n",
    "gamma_vals = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5]\n",
    "\n",
    "best_score = 0\n",
    "svm_hs = []\n",
    "\n",
    "for c in c_vals:\n",
    "    for gamma in gamma_vals:\n",
    "#         smoter = SMOTE(random_state=42)\n",
    "#         print('alpha', alpha)\n",
    "\n",
    "        print(c, gamma)\n",
    "        kf = KFold(n_splits=10)\n",
    "        avg_hs = []\n",
    "\n",
    "        for train_index,  test_index in kf.split(X_train_scaled):\n",
    "\n",
    "            x_train_svm, test_x = X_train_scaled.iloc[train_index.tolist(),:], X_train_scaled.iloc[test_index.tolist(),:]\n",
    "            y_train_1_svm, test_y_1 = np.array(Y_train_1)[train_index.tolist()], np.array(Y_train_1)[test_index.tolist()]\n",
    "            y_train_2_svm, test_y_2 = np.array(Y_train_2)[train_index.tolist()], np.array(Y_train_2)[test_index.tolist()]\n",
    "            y_train_3_svm, test_y_3 = np.array(Y_train_3)[train_index.tolist()], np.array(Y_train_3)[test_index.tolist()]\n",
    "\n",
    "            Y_test = test_y_1.tolist()\n",
    "            Y_test.extend(test_y_2.tolist())\n",
    "            Y_test.extend(test_y_3.tolist())\n",
    "            \n",
    "            clf1 = OneVsRestClassifier(SVC(C=c, gamma=gamma))\n",
    "            clf1.fit(x_train_svm, y_train_1_svm)\n",
    "            y_pred_1_svm = clf1.predict(test_x)\n",
    "#             print(hamming_loss(test_y_1, y_pred_1_svm))\n",
    "\n",
    "            clf2 = OneVsRestClassifier(SVC(C=c, gamma=gamma))\n",
    "            clf2.fit(x_train_svm, y_train_2_svm)\n",
    "            y_pred_2_svm = clf2.predict(test_x)\n",
    "#             print(hamming_loss(test_y_2, y_pred_2_svm))\n",
    "            \n",
    "            clf3 = OneVsRestClassifier(SVC(C=c, gamma=gamma))\n",
    "            clf3.fit(x_train_svm, y_train_3_svm)\n",
    "            y_pred_3_svm = clf3.predict(test_x)\n",
    "#             print(hamming_loss(test_y_3, y_pred_3_svm))\n",
    "\n",
    "            Y_pred = y_pred_1_svm.tolist()\n",
    "            Y_pred.extend(y_pred_2_svm)\n",
    "            Y_pred.extend(y_pred_3_svm)\n",
    "#               X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(x_train_xg, y_train_xg)\n",
    "\n",
    "#             x = XGBClassifier(random_state=42,reg_alpha=alpha, objective='binary:logistic', verbosity=0, n_jobs=-1, eval_metric='logloss')\n",
    "#             xgb.fit(X_train_fold_upsample,y_train_fold_upsample)\n",
    "\n",
    "#             pred = xgb.predict(test_x)\n",
    "#             f1 = f1_score(test_y, pred)\n",
    "        \n",
    "            avg_hs.append(1- hamming_loss(Y_test, Y_pred))\n",
    "\n",
    "        svm_hs.append(np.mean(avg_hs))\n",
    "        print(np.mean(avg_hs))\n",
    "        if np.mean(avg_hs) > best_score:\n",
    "            best_score = np.mean(avg_hs)\n",
    "            best_c = c\n",
    "            best_gamma = gamma\n",
    "\n",
    "    print(best_c, best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8df76adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C and gamma values are:  1 0.1\n"
     ]
    }
   ],
   "source": [
    "print('The best C and gamma values are: ', best_c, best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55bf278",
   "metadata": {},
   "source": [
    "iii. Repeat 1(b)ii with L1-penalized SVMs.3 Remember to standardize the attributes. Determine the weight of the SVM penalty using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "276514fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.8196337845940231\n",
      "0.001\n",
      "0.01\n",
      "0.9161309918268168\n",
      "0.01\n",
      "0.1\n",
      "0.9414862675797069\n",
      "0.1\n",
      "1\n",
      "0.9473771129834748\n",
      "1\n",
      "10\n",
      "0.9487681056518035\n",
      "10\n",
      "100\n",
      "0.9488343747041561\n",
      "100\n",
      "1000\n",
      "0.9489005122702936\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "c_vals = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "best_score = 0\n",
    "svm_hs = []\n",
    "\n",
    "for c in c_vals:\n",
    "    print(c)\n",
    "    kf = KFold(n_splits=10)\n",
    "    avg_hs = []\n",
    "\n",
    "    for train_index,  test_index in kf.split(X_train_scaled):\n",
    "\n",
    "        x_train_svm, test_x = X_train_scaled.iloc[train_index.tolist(),:], X_train_scaled.iloc[test_index.tolist(),:]\n",
    "        y_train_1_svm, test_y_1 = np.array(Y_train_1)[train_index.tolist()], np.array(Y_train_1)[test_index.tolist()]\n",
    "        y_train_2_svm, test_y_2 = np.array(Y_train_2)[train_index.tolist()], np.array(Y_train_2)[test_index.tolist()]\n",
    "        y_train_3_svm, test_y_3 = np.array(Y_train_3)[train_index.tolist()], np.array(Y_train_3)[test_index.tolist()]\n",
    "\n",
    "        Y_test = test_y_1.tolist()\n",
    "        Y_test.extend(test_y_2.tolist())\n",
    "        Y_test.extend(test_y_3.tolist())\n",
    "        \n",
    "        clf1 = LinearSVC(penalty='l1', C=c, dual=False)\n",
    "        clf1.fit(x_train_svm, y_train_1_svm)\n",
    "        y_pred_1_svm = clf1.predict(test_x)\n",
    "#             print(hamming_loss(test_y_1, y_pred_1_svm))\n",
    "\n",
    "        clf2 = LinearSVC(penalty='l1', C=c, dual=False)\n",
    "        clf2.fit(x_train_svm, y_train_2_svm)\n",
    "        y_pred_2_svm = clf2.predict(test_x)\n",
    "#             print(hamming_loss(test_y_2, y_pred_2_svm))\n",
    "\n",
    "        clf3 = LinearSVC(C=c, penalty='l1', dual=False)\n",
    "        clf3.fit(x_train_svm, y_train_3_svm)\n",
    "        y_pred_3_svm = clf3.predict(test_x)\n",
    "#             print(hamming_loss(test_y_3, y_pred_3_svm))\n",
    "\n",
    "        Y_pred = y_pred_1_svm.tolist()\n",
    "        Y_pred.extend(y_pred_2_svm)\n",
    "        Y_pred.extend(y_pred_3_svm)\n",
    "#               X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(x_train_xg, y_train_xg)\n",
    "\n",
    "#             x = XGBClassifier(random_state=42,reg_alpha=alpha, objective='binary:logistic', verbosity=0, n_jobs=-1, eval_metric='logloss')\n",
    "#             xgb.fit(X_train_fold_upsample,y_train_fold_upsample)\n",
    "\n",
    "#             pred = xgb.predict(test_x)\n",
    "#             f1 = f1_score(test_y, pred)\n",
    "\n",
    "        avg_hs.append(1- hamming_loss(Y_test, Y_pred))\n",
    "\n",
    "    svm_hs.append(np.mean(avg_hs))\n",
    "    print(np.mean(avg_hs))\n",
    "    if np.mean(avg_hs) > best_score:\n",
    "        best_score = np.mean(avg_hs)\n",
    "        best_c = c\n",
    "\n",
    "    print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0dce22e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C value is :  1000\n"
     ]
    }
   ],
   "source": [
    "print('The best C value is : ', best_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015cd88",
   "metadata": {},
   "source": [
    "iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy\n",
    "class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36029c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.8784082541786319\n",
      "0.001\n",
      "0.01\n",
      "0.9241463914923159\n",
      "0.01\n",
      "0.1\n",
      "0.9345365636866632\n",
      "0.1\n",
      "1\n",
      "0.9351324592129762\n",
      "1\n",
      "10\n",
      "0.9355298105546617\n",
      "10\n",
      "100\n",
      "0.9354635415023089\n",
      "10\n",
      "1000\n",
      "0.9353972724499565\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "c_vals = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "best_score = 0\n",
    "svm_hs = []\n",
    "\n",
    "for c in c_vals:\n",
    "    smoter = SMOTE(random_state=42)\n",
    "    print(c)\n",
    "    kf = KFold(n_splits=10)\n",
    "    avg_hs = []\n",
    "\n",
    "    for train_index,  test_index in kf.split(X_train_scaled):\n",
    "\n",
    "        x_train_svm, test_x = X_train_scaled.iloc[train_index.tolist(),:], X_train_scaled.iloc[test_index.tolist(),:]\n",
    "        y_train_1_svm, test_y_1 = np.array(Y_train_1)[train_index.tolist()], np.array(Y_train_1)[test_index.tolist()]\n",
    "        y_train_2_svm, test_y_2 = np.array(Y_train_2)[train_index.tolist()], np.array(Y_train_2)[test_index.tolist()]\n",
    "        y_train_3_svm, test_y_3 = np.array(Y_train_3)[train_index.tolist()], np.array(Y_train_3)[test_index.tolist()]\n",
    "\n",
    "        X_train_upsample_1, y_train_upsample_1 = smoter.fit_resample(x_train_svm, y_train_1_svm)\n",
    "        X_train_upsample_2, y_train_upsample_2 = smoter.fit_resample(x_train_svm, y_train_2_svm)\n",
    "        X_train_upsample_3, y_train_upsample_3 = smoter.fit_resample(x_train_svm, y_train_3_svm)\n",
    "        \n",
    "        Y_test = test_y_1.tolist()\n",
    "        Y_test.extend(test_y_2.tolist())\n",
    "        Y_test.extend(test_y_3.tolist())\n",
    "        \n",
    "        clf1 = LinearSVC(C=c, penalty='l1', dual=False)\n",
    "        clf1.fit(X_train_upsample_1, y_train_upsample_1)\n",
    "        y_pred_1_svm = clf1.predict(test_x)\n",
    "#             print(hamming_loss(test_y_1, y_pred_1_svm))\n",
    "\n",
    "        clf2 = LinearSVC(C=c, penalty='l1', dual=False)\n",
    "        clf2.fit(X_train_upsample_2, y_train_upsample_2)\n",
    "        y_pred_2_svm = clf2.predict(test_x)\n",
    "#             print(hamming_loss(test_y_2, y_pred_2_svm))\n",
    "\n",
    "        clf3 = LinearSVC(C=c, penalty='l1', dual=False)\n",
    "        clf3.fit(X_train_upsample_3, y_train_upsample_3)\n",
    "        y_pred_3_svm = clf3.predict(test_x)\n",
    "#             print(hamming_loss(test_y_3, y_pred_3_svm))\n",
    "\n",
    "        Y_pred = y_pred_1_svm.tolist()\n",
    "        Y_pred.extend(y_pred_2_svm)\n",
    "        Y_pred.extend(y_pred_3_svm)\n",
    "#               X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(x_train_xg, y_train_xg)\n",
    "\n",
    "#             x = XGBClassifier(random_state=42,reg_alpha=alpha, objective='binary:logistic', verbosity=0, n_jobs=-1, eval_metric='logloss')\n",
    "#             xgb.fit(X_train_fold_upsample,y_train_fold_upsample)\n",
    "\n",
    "#             pred = xgb.predict(test_x)\n",
    "#             f1 = f1_score(test_y, pred)\n",
    "\n",
    "        avg_hs.append(1- hamming_loss(Y_test, Y_pred))\n",
    "\n",
    "    svm_hs.append(np.mean(avg_hs))\n",
    "    print(np.mean(avg_hs))\n",
    "    if np.mean(avg_hs) > best_score:\n",
    "        best_score = np.mean(avg_hs)\n",
    "        best_c = c\n",
    "\n",
    "    print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18e4e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C value is :  10\n"
     ]
    }
   ],
   "source": [
    "print('The best C value is : ', best_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b515c",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "- Standardizing the features decreased the best penalty parameter value and the gamma parameter by orders of magnitude.\n",
    "- Using SMOTE to upsample and accomodate for class imbalance will result in a better model than the other classifiers.\n",
    "- Linear SVM has the highest C value since small C values give  a large number of missclassified examples as the data is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb38ee6",
   "metadata": {},
   "source": [
    "Q2: Monte-Carlo Simulation: Perform the following procedures 50 times, and report\n",
    "the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da842b40",
   "metadata": {},
   "source": [
    "(a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split\n",
    "the data into train and test, as we are not performing supervised learning in this\n",
    "exercise). Choose k in {1; 2; ... ; 50} automatically based on one of the methods\n",
    "provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any\n",
    "other method you know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77deb0f",
   "metadata": {},
   "source": [
    "(b) In each cluster, determine which family is the majority by reading the true labels.\n",
    "Repeat for genus and species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b70e48",
   "metadata": {},
   "source": [
    "(c) Now for each cluster you have a majority label triplet (family, genus, species).\n",
    "Calculate the average Hamming distance, Hamming score, and Hamming loss5\n",
    "between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "91cf075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_labels.drop(label_columns, axis=1)\n",
    "Y = df_labels[label_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7959733b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best k value is:  4\n",
      "Iteration:  0\n",
      "hamming loss:  0.22177438035672922\n",
      "hamming score:  0.7782256196432707\n",
      "hamming distance:  4787.0\n",
      "The best k value is:  4\n",
      "Iteration:  1\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  2\n",
      "hamming loss:  0.24526291406069028\n",
      "hamming score:  0.7547370859393097\n",
      "hamming distance:  5294.0\n",
      "The best k value is:  4\n",
      "Iteration:  3\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  4\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  5\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  6\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  7\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  8\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  9\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  10\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  11\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  12\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  13\n",
      "hamming loss:  0.22246930738939077\n",
      "hamming score:  0.7775306926106093\n",
      "hamming distance:  4802.0\n",
      "The best k value is:  4\n",
      "Iteration:  14\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  15\n",
      "hamming loss:  0.22246930738939077\n",
      "hamming score:  0.7775306926106093\n",
      "hamming distance:  4802.0\n",
      "The best k value is:  4\n",
      "Iteration:  16\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  17\n",
      "hamming loss:  0.23372712531850823\n",
      "hamming score:  0.7662728746814917\n",
      "hamming distance:  5045.0\n",
      "The best k value is:  4\n",
      "Iteration:  18\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  19\n",
      "hamming loss:  0.22177438035672922\n",
      "hamming score:  0.7782256196432707\n",
      "hamming distance:  4787.0\n",
      "The best k value is:  4\n",
      "Iteration:  20\n",
      "hamming loss:  0.23405142460041695\n",
      "hamming score:  0.765948575399583\n",
      "hamming distance:  5052.0\n",
      "The best k value is:  4\n",
      "Iteration:  21\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  22\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  23\n",
      "hamming loss:  0.22246930738939077\n",
      "hamming score:  0.7775306926106093\n",
      "hamming distance:  4802.0\n",
      "The best k value is:  4\n",
      "Iteration:  24\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  25\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  26\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  27\n",
      "hamming loss:  0.22246930738939077\n",
      "hamming score:  0.7775306926106093\n",
      "hamming distance:  4802.0\n",
      "The best k value is:  4\n",
      "Iteration:  28\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  29\n",
      "hamming loss:  0.22214500810748206\n",
      "hamming score:  0.7778549918925179\n",
      "hamming distance:  4795.0\n",
      "The best k value is:  4\n",
      "Iteration:  30\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  31\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  32\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  33\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  34\n",
      "hamming loss:  0.1860551308779245\n",
      "hamming score:  0.8139448691220755\n",
      "hamming distance:  4016.0000000000005\n",
      "The best k value is:  4\n",
      "Iteration:  35\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  36\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  37\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  38\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  39\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  40\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  41\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  42\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  43\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  44\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  45\n",
      "hamming loss:  0.22177438035672922\n",
      "hamming score:  0.7782256196432707\n",
      "hamming distance:  4787.0\n",
      "The best k value is:  4\n",
      "Iteration:  46\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  47\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n",
      "The best k value is:  4\n",
      "Iteration:  48\n",
      "hamming loss:  0.28005559416261294\n",
      "hamming score:  0.7199444058373871\n",
      "hamming distance:  6045.0\n",
      "The best k value is:  4\n",
      "Iteration:  49\n",
      "hamming loss:  0.2224229789205467\n",
      "hamming score:  0.7775770210794533\n",
      "hamming distance:  4801.0\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "score = []\n",
    "distance = []\n",
    "\n",
    "for m in range(50):\n",
    "    # We use silhouette score to find the best k\n",
    "    best_score = -1\n",
    "    for k in range(2, 51):\n",
    "        km = KMeans(n_clusters=k, random_state=m)\n",
    "        km.fit(X)\n",
    "        s_score = silhouette_score(X, km.labels_)\n",
    "#         print(k, s_score, m)\n",
    "        if s_score > best_score:\n",
    "            best_score = s_score\n",
    "            best_k = k\n",
    "    print('The best k value is: ', best_k)\n",
    "    final_km = KMeans(n_clusters=best_k, random_state=m)\n",
    "    final_km.fit(X)\n",
    "    cluster_labels = final_km.labels_\n",
    "    df_labels['cluster_label'] = cluster_labels\n",
    "    family_labels = df_labels.groupby('cluster_label')['Family'].value_counts().unstack().idxmax(axis=1).tolist()\n",
    "    genus_labels = df_labels.groupby('cluster_label')['Genus'].value_counts().unstack().idxmax(axis=1).tolist()\n",
    "    species_labels = df_labels.groupby('cluster_label')['Species'].value_counts().unstack().idxmax(axis=1).tolist()\n",
    "    \n",
    "    family_cluster = []\n",
    "    genus_cluster = []\n",
    "    species_cluster = []\n",
    "\n",
    "    for i in range(len(cluster_labels)):\n",
    "    #     print(i)\n",
    "        family_cluster.append(family_labels[cluster_labels[i]])\n",
    "        genus_cluster.append(genus_labels[cluster_labels[i]])\n",
    "        species_cluster.append(species_labels[cluster_labels[i]])\n",
    "\n",
    "    final_labels = family_cluster\n",
    "    final_labels.extend(genus_cluster)\n",
    "    final_labels.extend(species_cluster)\n",
    "    \n",
    "    Y_true = df_labels['Family'].tolist()\n",
    "    Y_true.extend(df_labels['Genus'].tolist())\n",
    "    Y_true.extend(df_labels['Species'].tolist())\n",
    "    \n",
    "    l = hamming_loss(Y_true, final_labels)\n",
    "    s = 1 - l\n",
    "    d = hamming(Y_true, final_labels) * len(Y_true)\n",
    "    loss.append(l)\n",
    "    score.append(s)\n",
    "    distance.append(d)\n",
    "    print('Iteration: ', m)\n",
    "    print('hamming loss: ', l)\n",
    "    print('hamming score: ', s)\n",
    "    print('hamming distance: ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c43b54e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2c9761d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average hamming distance, score and loss:  4829.06 0.7762770442436876 0.22372295575631226\n"
     ]
    }
   ],
   "source": [
    "print('Average hamming distance, score and loss: ', np.average(distance), np.average(score), np.average(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "89d6674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of hamming distance:  223.18632664211304\n"
     ]
    }
   ],
   "source": [
    "print('Standard deviation of hamming distance: ', np.std(distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32b7f2",
   "metadata": {},
   "source": [
    "ISLR 12.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06577494",
   "metadata": {},
   "source": [
    "![HW7.jpeg](HW7.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e596f4",
   "metadata": {},
   "source": [
    "(c) Cluster (1,2) and Cluster (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a41300",
   "metadata": {},
   "source": [
    "(d) Cluster ((1,2),3) and Cluster (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b64e3",
   "metadata": {},
   "source": [
    "![HW7_2.jpeg](HW7_2.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74561782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
